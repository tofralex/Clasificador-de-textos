{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **INGENIERÍA LINGÜÍSTICA. PRÁCTICA 2**\n",
    "\n",
    "En este cuaderno se encuentra toda la implementación de la práctica 2 de la asignatura de Ingeniería Lingüística del máster de Inteligencia Artifical de la UPM.\n",
    "\n",
    "El cuaderno se va a dividir en las siguientes partes:\n",
    "* 1. Imports\n",
    "* 2. Funciones de carga y procesado de documentos\n",
    "* 3. Implementación del algoritmo de clasificación\n",
    "* 4. Pruebas y evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "Se han utilizado diversas librerías de python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "from sklearn import preprocessing  # to normalise existing X\n",
    "from collections import Counter \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.data import load\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Funciones de carga y procesado de documentos\n",
    "\n",
    "En este apartado se detallan las funciones utilizadas para el preprocesado de los textos y la extracción de vocabulario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Parámetros\n",
    "\n",
    "Estos parámetros se utilizan en distintas funciones del código. Se ajustan para probar la mejora o empeoramiento de nuestro modelo. Estos parámetros son:\n",
    "\n",
    "* **Número mínimo de aparición**: son tres parámetros distintos que se usan para determinar cuál es el número mínimo de apariciones que debe tener una palabra en un conjunto de textos de un mismo tema para ser considerada importante o de interés para la clasificación\n",
    "    * **min_deportes**: número mínimo de veces que debe aparecer una palabra en los textos de deportes para que sea considerada\n",
    "    * **min_politica**: número mínimo de veces que debe aparecer una palabra en los textos de política para que sea considerada\n",
    "    * **min_salud**: número mínimo de veces que debe aparecer una palabra en los textos de salud para que sea considerada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parámetros usados\n",
    "min_deportes = 9\n",
    "min_politica = 10\n",
    "min_salud = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Funciones de procesado de documentos y obtención del glosario\n",
    "\n",
    "A continuación, se encuentran las funciones que se han utilizado para obtener el glosario de términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función de carga de los documentos\n",
    "def load_documents(filenames):\n",
    "    return [io.open(name, 'r', encoding='utf-8').read().replace(\"\\r\", \"\").replace(\"\\n\", \"\") for name in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función que comprueba la etiqueta original de cada documento\n",
    "#Usada para comprobar la bondad de nuestro clasificador y expresar el porcentaje de aciertos de cada documento\n",
    "def check_original_labels(filenames):\n",
    "    labels = []\n",
    "    \n",
    "    #Se aprovecha que cada documento tiene en su nombre la etiqueta a la que pertenece originalmente\n",
    "    for name in filenames:\n",
    "        if 'deportes' in name:\n",
    "            labels.append(0)\n",
    "        elif 'politica' in name:\n",
    "            labels.append(1)\n",
    "        elif 'salud' in name:\n",
    "            labels.append(2)\n",
    "        else:\n",
    "            labels.append(random.randint(0,3))\n",
    "            \n",
    "    #Devolución de la lista de etiquetas reales\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion tokenize (extracción de términos relevantes)\n",
    "def tokenize(text):\n",
    "    \n",
    "    #Lista de palabras vacías (stopwords): combinación entre archivo .json + lista de palabras extra + stopwords en español e inglés del paquete stopwords\n",
    "    add_stopwords = ['tan', 'través', 'aún', 'sun', 'ser', 'estar', 'tener', 'haber', 'efe', 'además', 'aun']\n",
    "    stopwords_json = json.load(open('stop_words_spanish.json',\"rb\"), encoding=\"utf-8\")\n",
    "    stopwords_spanish = stopwords.words('spanish') + add_stopwords + stopwords.words('english') + stopwords_json\n",
    "    \n",
    "    #Lista de signos de puntuación\n",
    "    non_words = list(punctuation)\n",
    "    non_words.extend(['¿', '¡','”', '“','«','º','ºc','»','‘','’','…','–','—','—','ª'])\n",
    "    non_words.extend(map(str,range(10)))\n",
    "    \n",
    "    #Obtención de los términos de los textos sin los signos de puntuación y demás elementos que no son palabras\n",
    "    text = ''.join([c.lower() for c in text if c not in non_words])\n",
    "    \n",
    "    #Eliminación de las stopwords de la lista anterior y devolución de la lista final de términos\n",
    "    tokens_with_stopwords = re.findall(\"[A-Za-zÀ-ÖØ-öø-ÿ]{3,}\", text)\n",
    "    tokens = [token for token in tokens_with_stopwords if token not in stopwords_spanish]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion usada para contar el número de apariciones de las palabras en un texto\n",
    "def count_words(documents):\n",
    "    \n",
    "    #Lista de palabras encontradas en los documentos tras pasar por el tokenizer\n",
    "    wordstring = ''\n",
    "    for document in documents:\n",
    "        wordstring += \" \".join(str(item) for item in tokenize(document))\n",
    "    \n",
    "    wordlist = wordstring.split()\n",
    "    \n",
    "    #Lista de frecuencias de las palabras\n",
    "    wordfreq = []\n",
    "    for w in wordlist:\n",
    "        wordfreq.append(wordlist.count(w))\n",
    "\n",
    "    return wordlist, wordfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Glosario de deportes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga de los documentos pertenecientes al apartado de deportes\n",
    "deportes = load_documents(sorted(glob.glob('Deportes/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actividad', 'actividades', 'anna', 'atlético', 'años', 'baloncesto', 'china', 'competiciones', 'competición', 'conjunto', 'conjuntos', 'cuarto', 'cuartos', 'deporte', 'deportes', 'deportiva', 'deportivas', 'deportivo', 'deportivos', 'diego', 'encuentro', 'encuentros', 'equipo', 'equipos', 'españa', 'español', 'española', 'españolas', 'españoles', 'final', 'finales', 'frente', 'frentes', 'grupo', 'grupos', 'historia', 'historias', 'hora', 'horas', 'juan', 'jugador', 'jugadora', 'jugadoras', 'jugadores', 'liga', 'ligas', 'madrid', 'maradona', 'minuto', 'minutos', 'partido', 'partidos', 'popular', 'presidente', 'presidentes', 'prueba', 'pruebas', 'real', 'reales', 'rival', 'rivales', 'samaranch']\n"
     ]
    }
   ],
   "source": [
    "#Se seleccionan los documentos de test para hacer el glosario sólo con ellos\n",
    "wordlist, wordfreq = count_words(deportes)\n",
    "\n",
    "#Se forma la lista de términos\n",
    "deportes_new_vocab = []\n",
    "for word, count in zip(wordlist,wordfreq):\n",
    "    \n",
    "    #Se comprueba que los términos aparezcan un mínimo de veces y no estén ya incluídos previamente en el glosario\n",
    "    if(count >= min_deportes and word not in deportes_new_vocab):\n",
    "        \n",
    "        #La lista final será un glosario de cada tema sin palabras repetidas\n",
    "        deportes_new_vocab.append(word)\n",
    "\n",
    "variab_deportes = ['actividades', 'competiciones', 'conjuntos', 'cuartos', 'deportes', 'deportiva', 'deportivas', \n",
    "                   'deportivos', 'encuentros', 'española', 'españolas', 'españoles', 'finales', 'frentes', 'grupos', \n",
    "                   'historias', 'hora', 'jugador', 'jugadora', 'jugadoras', 'ligas', 'minuto', 'partidos', 'presidentes', \n",
    "                   'pruebas', 'reales', 'rivales']\n",
    "deportes_new_vocab = deportes_new_vocab + variab_deportes\n",
    "\n",
    "deportes_new_vocab.sort()\n",
    "print(deportes_new_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Glosario de política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga de los documentos pertenecientes al apartado de política\n",
    "politica = load_documents(sorted(glob.glob('Política/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acuerdo', 'acuerdos', 'audiencia', 'audiencias', 'año', 'años', 'brexit', 'británica', 'británicas', 'británico', 'británicos', 'bruselas', 'carlos', 'casado', 'caso', 'casos', 'cgpj', 'comisiones', 'comisión', 'congreso', 'congresos', 'consejo', 'consejos', 'crisis', 'día', 'días', 'empresa', 'empresas', 'enero', 'españa', 'europea', 'europeas', 'europeo', 'europeos', 'fernández', 'formaciones', 'formación', 'funciones', 'función', 'general', 'generales', 'gobierno', 'gobiernos', 'interior', 'israel', 'johnson', 'judicial', 'judiciales', 'jueces', 'juez', 'jueza', 'juezas', 'ley', 'leyes', 'londres', 'lunes', 'madrid', 'mercado', 'mercados', 'mes', 'meses', 'ministra', 'ministras', 'ministro', 'ministros', 'moncloa', 'nacional', 'nacionales', 'oposiciones', 'oposición', 'pablo', 'pacto', 'pactos', 'partido', 'partidos', 'país', 'países', 'policía', 'policías', 'política', 'presidente', 'problema', 'problemas', 'psoe', 'reina', 'reinas', 'reino', 'renovar', 'rey', 'reyes', 'saudí', 'saudíes', 'semana', 'supremo', 'sánchez', 'tribunal', 'tribunales', 'unidas', 'unido']\n"
     ]
    }
   ],
   "source": [
    "#Se seleccionan los documentos de test para hacer el glosario sólo con ellos\n",
    "wordlist, wordfreq = count_words(politica)\n",
    "\n",
    "#Se forma la lista de términos\n",
    "politica_new_vocab = []\n",
    "\n",
    "for word, count in zip(wordlist,wordfreq):\n",
    "    \n",
    "    #Se comprueba que los términos aparezcan un mínimo de veces y no estén ya incluídos previamente en el glosario\n",
    "    if(count >= min_politica and word not in politica_new_vocab):\n",
    "        \n",
    "        #La lista final será un glosario de cada tema sin palabras repetidas\n",
    "        politica_new_vocab.append(word)\n",
    "\n",
    "\n",
    "variab_politica = ['acuerdos', 'audiencias', 'británicos', 'británicas', 'británica', 'casos', 'comisiones', 'congresos', \n",
    "                   'consejos', 'día', 'empresa', 'europeas', 'europeos', 'formaciones', 'función', 'generales', \n",
    "                   'gobiernos', 'judiciales', 'jueces', 'jueza', 'juezas', 'leyes', 'mercados', 'meses', 'ministros', \n",
    "                   'ministra', 'ministras', 'nacionales', 'oposiciones', 'pactos', 'partidos', 'países', 'policías', \n",
    "                   'problemas', 'reina', 'reyes', 'reinas', 'saudíes', 'tribunales']\n",
    "politica_new_vocab = politica_new_vocab + variab_politica\n",
    "\n",
    "politica_new_vocab.sort()\n",
    "print(politica_new_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Glosario de salud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga de los documentos pertenecientes al apartado de salud\n",
    "salud = load_documents(sorted(glob.glob('Salud/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asociaciones', 'asociación', 'atención', 'año', 'años', 'carlos', 'caso', 'casos', 'casos', 'covid', 'cáncer', 'diagnóstico', 'diagnósticos', 'dolor', 'dolores', 'día', 'días', 'ecuador', 'enfermedad', 'enfermedades', 'españa', 'española', 'evitar', 'experta', 'expertas', 'experto', 'expertos', 'explica', 'falta', 'forma', 'frenillo', 'gripe', 'hospital', 'hospitales', 'indica', 'infecciones', 'infección', 'lucha', 'luchas', 'malaria', 'mental', 'mentales', 'mes', 'meses', 'migraña', 'millones', 'mujer', 'mujeres', 'mundial', 'mundiales', 'niño', 'niños', 'objetivo', 'objetivos', 'paciente', 'pacientes', 'pandemia', 'pandemias', 'patología', 'patologías', 'país', 'países', 'persona', 'personas', 'piel', 'prevención', 'problema', 'problemas', 'profesional', 'profesionales', 'protección', 'recurso', 'recursos', 'salud', 'sanitario', 'sanitarios', 'sida', 'sociedad', 'solar', 'síntoma', 'síntomas', 'tipo', 'tipos', 'trastorno', 'trastornos', 'tratamiento', 'tratamientos', 'universidad', 'universidades', 'vida', 'vih', 'virus']\n"
     ]
    }
   ],
   "source": [
    "#Se seleccionan los documentos de test para hacer el glosario sólo con ellos\n",
    "wordlist, wordfreq = count_words(salud)\n",
    "\n",
    "#Se forma la lista de términos\n",
    "salud_new_vocab = []\n",
    "for word, count in zip(wordlist,wordfreq):\n",
    "    \n",
    "    #Se comprueba que los términos aparezcan un mínimo de veces y no estén ya incluídos previamente en el glosario\n",
    "    if(count >= min_salud and word not in salud_new_vocab):\n",
    "        \n",
    "        #La lista final será un glosario de cada tema sin palabras repetidas\n",
    "        salud_new_vocab.append(word)\n",
    "\n",
    "variab_salud = ['asociaciones', 'casos', 'diagnósticos', 'dolores', 'experto', 'experta', 'expertas', 'hospitales', 'infección', \n",
    "                'luchas', 'mentales', 'mes', 'mujer', 'mundiales', 'niño', 'objetivos', 'pandemias', 'patologías', \n",
    "                'persona', 'profesional', 'recurso', 'sanitario', 'síntoma', 'tipos', 'trastorno', 'universidades']\n",
    "salud_new_vocab = salud_new_vocab + variab_salud\n",
    "\n",
    "salud_new_vocab.sort()\n",
    "print(salud_new_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Glosario final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actividad', 'actividades', 'acuerdo', 'acuerdos', 'anna', 'asociaciones', 'asociación', 'atención', 'atlético', 'audiencia', 'audiencias', 'año', 'años', 'baloncesto', 'brexit', 'británica', 'británicas', 'británico', 'británicos', 'bruselas', 'carlos', 'casado', 'caso', 'casos', 'cgpj', 'china', 'comisiones', 'comisión', 'competiciones', 'competición', 'congreso', 'congresos', 'conjunto', 'conjuntos', 'consejo', 'consejos', 'covid', 'crisis', 'cuarto', 'cuartos', 'cáncer', 'deporte', 'deportes', 'deportiva', 'deportivas', 'deportivo', 'deportivos', 'diagnóstico', 'diagnósticos', 'diego', 'dolor', 'dolores', 'día', 'días', 'ecuador', 'empresa', 'empresas', 'encuentro', 'encuentros', 'enero', 'enfermedad', 'enfermedades', 'equipo', 'equipos', 'españa', 'español', 'española', 'españolas', 'españoles', 'europea', 'europeas', 'europeo', 'europeos', 'evitar', 'experta', 'expertas', 'experto', 'expertos', 'explica', 'falta', 'fernández', 'final', 'finales', 'forma', 'formaciones', 'formación', 'frenillo', 'frente', 'frentes', 'funciones', 'función', 'general', 'generales', 'gobierno', 'gobiernos', 'gripe', 'grupo', 'grupos', 'historia', 'historias', 'hora', 'horas', 'hospital', 'hospitales', 'indica', 'infecciones', 'infección', 'interior', 'israel', 'johnson', 'juan', 'judicial', 'judiciales', 'jueces', 'juez', 'jueza', 'juezas', 'jugador', 'jugadora', 'jugadoras', 'jugadores', 'ley', 'leyes', 'liga', 'ligas', 'londres', 'lucha', 'luchas', 'lunes', 'madrid', 'malaria', 'maradona', 'mental', 'mentales', 'mercado', 'mercados', 'mes', 'meses', 'migraña', 'millones', 'ministra', 'ministras', 'ministro', 'ministros', 'minuto', 'minutos', 'moncloa', 'mujer', 'mujeres', 'mundial', 'mundiales', 'nacional', 'nacionales', 'niño', 'niños', 'objetivo', 'objetivos', 'oposiciones', 'oposición', 'pablo', 'paciente', 'pacientes', 'pacto', 'pactos', 'pandemia', 'pandemias', 'partido', 'partidos', 'patología', 'patologías', 'país', 'países', 'persona', 'personas', 'piel', 'policía', 'policías', 'política', 'popular', 'presidente', 'presidentes', 'prevención', 'problema', 'problemas', 'profesional', 'profesionales', 'protección', 'prueba', 'pruebas', 'psoe', 'real', 'reales', 'recurso', 'recursos', 'reina', 'reinas', 'reino', 'renovar', 'rey', 'reyes', 'rival', 'rivales', 'salud', 'samaranch', 'sanitario', 'sanitarios', 'saudí', 'saudíes', 'semana', 'sida', 'sociedad', 'solar', 'supremo', 'sánchez', 'síntoma', 'síntomas', 'tipo', 'tipos', 'trastorno', 'trastornos', 'tratamiento', 'tratamientos', 'tribunal', 'tribunales', 'unidas', 'unido', 'universidad', 'universidades', 'vida', 'vih', 'virus']\n"
     ]
    }
   ],
   "source": [
    "#La lista \"vocabulario\" será el resultado de sumar las tres listas anteriores sin palabras repetidas\n",
    "vocabulario = deportes_new_vocab\n",
    "vocabulario = vocabulario + [token for token in politica_new_vocab if token not in vocabulario]\n",
    "vocabulario = vocabulario + [token for token in salud_new_vocab if token not in vocabulario]\n",
    "vocabulario.sort()\n",
    "\n",
    "print(vocabulario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementación del algoritmo de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conjunto de documentos de entrenamiento del clasificador\n",
    "X_Train = deportes + politica + salud\n",
    "Y_Train = [0] * len(deportes) + [1] * len(politica) + [2] * len(salud)\n",
    "\n",
    "#Conjunto de documentos de test (prueba) del clasificador\n",
    "filenames_test = glob.glob('Test/*')\n",
    "X_Test = load_documents(filenames_test)    \n",
    "Y_Test = check_original_labels(filenames_test)\n",
    "\n",
    "#Se aleatorizan todos los documentos, tanto los de entrenamiento como los osmetidos a las posteriores pruebas\n",
    "c = list(zip(X_Train, Y_Train))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "X_Train, Y_Train = zip(*c)\n",
    "\n",
    "d = list(zip(X_Test, Y_Test, filenames_test))\n",
    "\n",
    "random.shuffle(d)\n",
    "\n",
    "X_Test, Y_Test, filenames_test = zip(*d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se aplica la métrica tf-idf vista en clase\n",
    "vectorizer = TfidfVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                vocabulary = vocabulario)\n",
    "\n",
    "X_Train_TF = vectorizer.fit_transform(X_Train)\n",
    "X_Test_TF =  vectorizer.transform(X_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. KNN y resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+----------+\n",
      "|      Documento       | Probabilidad (%) |   Tema   |\n",
      "+----------------------+------------------+----------+\n",
      "| 4-test-politica.txt  |      100.0       | Politica |\n",
      "|  10-test-salud.txt   |      100.0       |  Salud   |\n",
      "|  13-test-salud.txt   |      100.0       |  Salud   |\n",
      "| 12-test-politica.txt |      100.0       | Politica |\n",
      "| 3-test-politica.txt  |       75.0       | Politica |\n",
      "|  14-test-salud.txt   |      100.0       |  Salud   |\n",
      "| 7-test-deportes.txt  |       75.0       | Deportes |\n",
      "| 9-test-politica.txt  |      100.0       | Politica |\n",
      "| 14-test-deportes.txt |      100.0       | Deportes |\n",
      "| 5-test-deportes.txt  |      100.0       | Deportes |\n",
      "|   8-test-salud.txt   |       75.0       |  Salud   |\n",
      "| 2-test-deportes.txt  |       75.0       | Deportes |\n",
      "| 6-test-deportes.txt  |      100.0       | Deportes |\n",
      "| 13-test-politica.txt |       75.0       | Politica |\n",
      "| 8-test-politica.txt  |      100.0       | Politica |\n",
      "|  11-test-salud.txt   |      100.0       |  Salud   |\n",
      "|   9-test-salud.txt   |      100.0       |  Salud   |\n",
      "| 8-test-deportes.txt  |      100.0       | Deportes |\n",
      "|   2-test-salud.txt   |      100.0       |  Salud   |\n",
      "| 3-test-deportes.txt  |      100.0       | Deportes |\n",
      "| 2-test-politica.txt  |      100.0       | Politica |\n",
      "| 10-test-politica.txt |       50.0       | Deportes |\n",
      "|   6-test-salud.txt   |      100.0       |  Salud   |\n",
      "| 5-test-politica.txt  |      100.0       | Politica |\n",
      "|   4-test-salud.txt   |      100.0       |  Salud   |\n",
      "| 6-test-politica.txt  |      100.0       | Politica |\n",
      "| 11-test-politica.txt |       50.0       | Politica |\n",
      "| 7-test-politica.txt  |      100.0       | Politica |\n",
      "| 10-test-deportes.txt |      100.0       | Deportes |\n",
      "| 15-test-politica.txt |      100.0       | Politica |\n",
      "| 15-test-deportes.txt |       75.0       | Deportes |\n",
      "| 1-test-deportes.txt  |      100.0       | Deportes |\n",
      "| 14-test-politica.txt |      100.0       | Politica |\n",
      "| 9-test-deportes.txt  |      100.0       | Deportes |\n",
      "|   3-test-salud.txt   |      100.0       |  Salud   |\n",
      "|  12-test-salud.txt   |       75.0       |  Salud   |\n",
      "| 13-test-deportes.txt |      100.0       | Deportes |\n",
      "|   1-test-salud.txt   |      100.0       |  Salud   |\n",
      "|   7-test-salud.txt   |      100.0       |  Salud   |\n",
      "|  15-test-salud.txt   |      100.0       |  Salud   |\n",
      "| 12-test-deportes.txt |      100.0       | Deportes |\n",
      "| 1-test-politica.txt  |      100.0       | Politica |\n",
      "| 11-test-deportes.txt |       75.0       | Deportes |\n",
      "|   5-test-salud.txt   |      100.0       |  Salud   |\n",
      "| 4-test-deportes.txt  |       75.0       | Deportes |\n",
      "+----------------------+------------------+----------+\n",
      "Train Accuracy: 100.0 %\n",
      "Test Accuracy: 97.77777777777777 %\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "X_Train_TF = preprocessing.normalize(X_Train_TF)\n",
    "X_Test_TF = preprocessing.normalize(X_Test_TF)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=n,weights='uniform', metric='euclidean').fit(X_Train_TF, Y_Train)\n",
    "\n",
    "score_train = knn.score(X_Train_TF,Y_Train)\n",
    "proba_test = knn.predict_proba(X_Test_TF)\n",
    "score_test = knn.score(X_Test_TF,Y_Test)\n",
    "\n",
    "\n",
    "temas = {0:'Deportes', 1:'Politica', 2:'Salud'}\n",
    "\n",
    "\n",
    "t = PrettyTable(['Documento', 'Probabilidad (%)', 'Tema'])\n",
    "for document_name, proba in zip(filenames_test,proba_test):\n",
    "    t.add_row([document_name.replace(\"Test\\\\\", \"\"), max(proba)*100, temas[np.argmax(proba)]])\n",
    "\n",
    "print(t)\n",
    "print('Train Accuracy:', score_train * 100, '%')\n",
    "print('Test Accuracy:', score_test * 100, '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
